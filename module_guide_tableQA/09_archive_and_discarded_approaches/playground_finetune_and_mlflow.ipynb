{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPrYJOn81f0D"
      },
      "source": [
        "## First Application Of Tapas Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_excel(\"DataExtraction/MS_IS_all_modules.xlsx\")\n",
        "data.head()\n",
        "\n",
        "\n",
        "from transformers import TapasForQuestionAnswering, TapasTokenizer\n",
        "from transformers import pipeline\n",
        "\n",
        "tqa = pipeline(task='table-question-answering', model='google/tapas-base-finetuned-wtq')\n",
        "\n",
        "\n",
        "table = pd.read_excel(\"DataExtraction/MS_IS_all_modules.xlsx\")\n",
        "table = table.astype(str)\n",
        "\n",
        "query = [\"What's the module title of the module with abbreviation 12-IV-161-m01?\"]\n",
        "query = [\"What ist the workload of Module title IT-Management?\"]\n",
        "answer = tqa(table=table, query=query)\n",
        "print(answer['answer'].strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FT Versuch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'int' object is not subscriptable",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[101], line 28\u001b[0m\n\u001b[0;32m     24\u001b[0m answer_coordinates \u001b[39m=\u001b[39m [(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)]  \u001b[39m# Example coordinates for the answer\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[39m# Tokenize the training data using the TapasTokenizer\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m inputs \u001b[39m=\u001b[39m tokenizer(\n\u001b[0;32m     29\u001b[0m     table\u001b[39m=\u001b[39;49mdf,\n\u001b[0;32m     30\u001b[0m     queries\u001b[39m=\u001b[39;49mqueries,\n\u001b[0;32m     31\u001b[0m     answer_text\u001b[39m=\u001b[39;49manswer_text,\n\u001b[0;32m     32\u001b[0m     answer_coordinates\u001b[39m=\u001b[39;49manswer_coordinates,\n\u001b[0;32m     33\u001b[0m     padding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmax_length\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     34\u001b[0m     truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     35\u001b[0m     return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m     36\u001b[0m )\n\u001b[0;32m     38\u001b[0m \u001b[39m# Define the training arguments\u001b[39;00m\n\u001b[0;32m     39\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[0;32m     40\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./results\u001b[39m\u001b[39m'\u001b[39m,          \u001b[39m# output directory\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     num_train_epochs\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,              \u001b[39m# total number of training epochs\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     47\u001b[0m     logging_steps\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,\n\u001b[0;32m     48\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\michi\\Anaconda3\\envs\\enterpriseai2\\lib\\site-packages\\transformers\\models\\tapas\\tokenization_tapas.py:650\u001b[0m, in \u001b[0;36mTapasTokenizer.__call__\u001b[1;34m(self, table, queries, answer_coordinates, answer_text, add_special_tokens, padding, truncation, max_length, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    647\u001b[0m is_batched \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(queries, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m))\n\u001b[0;32m    649\u001b[0m \u001b[39mif\u001b[39;00m is_batched:\n\u001b[1;32m--> 650\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_encode_plus(\n\u001b[0;32m    651\u001b[0m         table\u001b[39m=\u001b[39;49mtable,\n\u001b[0;32m    652\u001b[0m         queries\u001b[39m=\u001b[39;49mqueries,\n\u001b[0;32m    653\u001b[0m         answer_coordinates\u001b[39m=\u001b[39;49manswer_coordinates,\n\u001b[0;32m    654\u001b[0m         answer_text\u001b[39m=\u001b[39;49manswer_text,\n\u001b[0;32m    655\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[0;32m    656\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[0;32m    657\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[0;32m    658\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[0;32m    659\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[0;32m    660\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[0;32m    661\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[0;32m    662\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[0;32m    663\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[0;32m    664\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[0;32m    665\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[0;32m    666\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[0;32m    667\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    668\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    669\u001b[0m     )\n\u001b[0;32m    670\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    671\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_plus(\n\u001b[0;32m    672\u001b[0m         table\u001b[39m=\u001b[39mtable,\n\u001b[0;32m    673\u001b[0m         query\u001b[39m=\u001b[39mqueries,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    689\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    690\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\michi\\Anaconda3\\envs\\enterpriseai2\\lib\\site-packages\\transformers\\models\\tapas\\tokenization_tapas.py:768\u001b[0m, in \u001b[0;36mTapasTokenizer.batch_encode_plus\u001b[1;34m(self, table, queries, answer_coordinates, answer_text, add_special_tokens, padding, truncation, max_length, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[39mif\u001b[39;00m return_offsets_mapping:\n\u001b[0;32m    762\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    763\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    764\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    765\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtransformers.PreTrainedTokenizerFast.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    766\u001b[0m     )\n\u001b[1;32m--> 768\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_encode_plus(\n\u001b[0;32m    769\u001b[0m     table\u001b[39m=\u001b[39;49mtable,\n\u001b[0;32m    770\u001b[0m     queries\u001b[39m=\u001b[39;49mqueries,\n\u001b[0;32m    771\u001b[0m     answer_coordinates\u001b[39m=\u001b[39;49manswer_coordinates,\n\u001b[0;32m    772\u001b[0m     answer_text\u001b[39m=\u001b[39;49manswer_text,\n\u001b[0;32m    773\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[0;32m    774\u001b[0m     padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[0;32m    775\u001b[0m     truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[0;32m    776\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[0;32m    777\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[0;32m    778\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[0;32m    779\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[0;32m    780\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[0;32m    781\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[0;32m    782\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[0;32m    783\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[0;32m    784\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[0;32m    785\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    786\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    787\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\michi\\Anaconda3\\envs\\enterpriseai2\\lib\\site-packages\\transformers\\models\\tapas\\tokenization_tapas.py:835\u001b[0m, in \u001b[0;36mTapasTokenizer._batch_encode_plus\u001b[1;34m(self, table, queries, answer_coordinates, answer_text, add_special_tokens, padding, truncation, max_length, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    832\u001b[0m     queries[idx] \u001b[39m=\u001b[39m query\n\u001b[0;32m    833\u001b[0m     queries_tokens\u001b[39m.\u001b[39mappend(query_tokens)\n\u001b[1;32m--> 835\u001b[0m batch_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_prepare_for_model(\n\u001b[0;32m    836\u001b[0m     table,\n\u001b[0;32m    837\u001b[0m     queries,\n\u001b[0;32m    838\u001b[0m     tokenized_table\u001b[39m=\u001b[39;49mtable_tokens,\n\u001b[0;32m    839\u001b[0m     queries_tokens\u001b[39m=\u001b[39;49mqueries_tokens,\n\u001b[0;32m    840\u001b[0m     answer_coordinates\u001b[39m=\u001b[39;49manswer_coordinates,\n\u001b[0;32m    841\u001b[0m     padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[0;32m    842\u001b[0m     truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[0;32m    843\u001b[0m     answer_text\u001b[39m=\u001b[39;49manswer_text,\n\u001b[0;32m    844\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[0;32m    845\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[0;32m    846\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[0;32m    847\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[0;32m    848\u001b[0m     prepend_batch_axis\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    849\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[0;32m    850\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[0;32m    851\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[0;32m    852\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[0;32m    853\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[0;32m    854\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    855\u001b[0m )\n\u001b[0;32m    857\u001b[0m \u001b[39mreturn\u001b[39;00m BatchEncoding(batch_outputs)\n",
            "File \u001b[1;32mc:\\Users\\michi\\Anaconda3\\envs\\enterpriseai2\\lib\\site-packages\\transformers\\models\\tapas\\tokenization_tapas.py:890\u001b[0m, in \u001b[0;36mTapasTokenizer._batch_prepare_for_model\u001b[1;34m(self, raw_table, raw_queries, tokenized_table, queries_tokens, answer_coordinates, answer_text, add_special_tokens, padding, truncation, max_length, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, prepend_batch_axis, **kwargs)\u001b[0m\n\u001b[0;32m    888\u001b[0m \u001b[39mfor\u001b[39;00m index, example \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(raw_queries, queries_tokens, answer_coordinates, answer_text)):\n\u001b[0;32m    889\u001b[0m     raw_query, query_tokens, answer_coords, answer_txt \u001b[39m=\u001b[39m example\n\u001b[1;32m--> 890\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare_for_model(\n\u001b[0;32m    891\u001b[0m         raw_table,\n\u001b[0;32m    892\u001b[0m         raw_query,\n\u001b[0;32m    893\u001b[0m         tokenized_table\u001b[39m=\u001b[39;49mtokenized_table,\n\u001b[0;32m    894\u001b[0m         query_tokens\u001b[39m=\u001b[39;49mquery_tokens,\n\u001b[0;32m    895\u001b[0m         answer_coordinates\u001b[39m=\u001b[39;49manswer_coords,\n\u001b[0;32m    896\u001b[0m         answer_text\u001b[39m=\u001b[39;49manswer_txt,\n\u001b[0;32m    897\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[0;32m    898\u001b[0m         padding\u001b[39m=\u001b[39;49mPaddingStrategy\u001b[39m.\u001b[39;49mDO_NOT_PAD\u001b[39m.\u001b[39;49mvalue,  \u001b[39m# we pad in batch afterwards\u001b[39;49;00m\n\u001b[0;32m    899\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[0;32m    900\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[0;32m    901\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,  \u001b[39m# we pad in batch afterwards\u001b[39;49;00m\n\u001b[0;32m    902\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,  \u001b[39m# we pad in batch afterwards\u001b[39;49;00m\n\u001b[0;32m    903\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[0;32m    904\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[0;32m    905\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[0;32m    906\u001b[0m         return_tensors\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,  \u001b[39m# We convert the whole batch to tensors at the end\u001b[39;49;00m\n\u001b[0;32m    907\u001b[0m         prepend_batch_axis\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    908\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    909\u001b[0m         prev_answer_coordinates\u001b[39m=\u001b[39;49manswer_coordinates[index \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m] \u001b[39mif\u001b[39;49;00m index \u001b[39m!=\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    910\u001b[0m         prev_answer_text\u001b[39m=\u001b[39;49manswer_text[index \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m] \u001b[39mif\u001b[39;49;00m index \u001b[39m!=\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    911\u001b[0m     )\n\u001b[0;32m    913\u001b[0m     \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m outputs\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    914\u001b[0m         \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m batch_outputs:\n",
            "File \u001b[1;32mc:\\Users\\michi\\Anaconda3\\envs\\enterpriseai2\\lib\\site-packages\\transformers\\models\\tapas\\tokenization_tapas.py:1248\u001b[0m, in \u001b[0;36mTapasTokenizer.prepare_for_model\u001b[1;34m(self, raw_table, raw_query, tokenized_table, query_tokens, answer_coordinates, answer_text, add_special_tokens, padding, truncation, max_length, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, prepend_batch_axis, **kwargs)\u001b[0m\n\u001b[0;32m   1245\u001b[0m     encoded_inputs[\u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m attention_mask\n\u001b[0;32m   1247\u001b[0m \u001b[39mif\u001b[39;00m answer_coordinates \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m answer_text \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1248\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_answer_ids(column_ids, row_ids, table_data, answer_text, answer_coordinates)\n\u001b[0;32m   1249\u001b[0m     numeric_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_numeric_values(raw_table, column_ids, row_ids)\n\u001b[0;32m   1250\u001b[0m     numeric_values_scale \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_numeric_values_scale(raw_table, column_ids, row_ids)\n",
            "File \u001b[1;32mc:\\Users\\michi\\Anaconda3\\envs\\enterpriseai2\\lib\\site-packages\\transformers\\models\\tapas\\tokenization_tapas.py:1825\u001b[0m, in \u001b[0;36mTapasTokenizer.get_answer_ids\u001b[1;34m(self, column_ids, row_ids, tokenized_table, answer_texts_question, answer_coordinates_question)\u001b[0m\n\u001b[0;32m   1818\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_answer_coordinates:\n\u001b[0;32m   1819\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_find_answer_ids_from_answer_texts(\n\u001b[0;32m   1820\u001b[0m         column_ids,\n\u001b[0;32m   1821\u001b[0m         row_ids,\n\u001b[0;32m   1822\u001b[0m         tokenized_table,\n\u001b[0;32m   1823\u001b[0m         answer_texts\u001b[39m=\u001b[39m[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenize(at) \u001b[39mfor\u001b[39;00m at \u001b[39min\u001b[39;00m answer_texts_question],\n\u001b[0;32m   1824\u001b[0m     )\n\u001b[1;32m-> 1825\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_answer_ids(column_ids, row_ids, answer_coordinates_question)\n",
            "File \u001b[1;32mc:\\Users\\michi\\Anaconda3\\envs\\enterpriseai2\\lib\\site-packages\\transformers\\models\\tapas\\tokenization_tapas.py:1811\u001b[0m, in \u001b[0;36mTapasTokenizer._get_answer_ids\u001b[1;34m(self, column_ids, row_ids, answer_coordinates)\u001b[0m\n\u001b[0;32m   1809\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_answer_ids\u001b[39m(\u001b[39mself\u001b[39m, column_ids, row_ids, answer_coordinates):\n\u001b[0;32m   1810\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Maps answer coordinates of a question to token indexes.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1811\u001b[0m     answer_ids, missing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_all_answer_ids(column_ids, row_ids, answer_coordinates)\n\u001b[0;32m   1813\u001b[0m     \u001b[39mif\u001b[39;00m missing_count:\n\u001b[0;32m   1814\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find all answers\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\michi\\Anaconda3\\envs\\enterpriseai2\\lib\\site-packages\\transformers\\models\\tapas\\tokenization_tapas.py:1737\u001b[0m, in \u001b[0;36mTapasTokenizer._get_all_answer_ids\u001b[1;34m(self, column_ids, row_ids, answer_coordinates)\u001b[0m\n\u001b[0;32m   1733\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_to_coordinates\u001b[39m(answer_coordinates_question):\n\u001b[0;32m   1734\u001b[0m     \u001b[39mreturn\u001b[39;00m [(coords[\u001b[39m1\u001b[39m], coords[\u001b[39m0\u001b[39m]) \u001b[39mfor\u001b[39;00m coords \u001b[39min\u001b[39;00m answer_coordinates_question]\n\u001b[0;32m   1736\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_all_answer_ids_from_coordinates(\n\u001b[1;32m-> 1737\u001b[0m     column_ids, row_ids, answers_list\u001b[39m=\u001b[39m(_to_coordinates(answer_coordinates))\n\u001b[0;32m   1738\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\michi\\Anaconda3\\envs\\enterpriseai2\\lib\\site-packages\\transformers\\models\\tapas\\tokenization_tapas.py:1734\u001b[0m, in \u001b[0;36mTapasTokenizer._get_all_answer_ids.<locals>._to_coordinates\u001b[1;34m(answer_coordinates_question)\u001b[0m\n\u001b[0;32m   1733\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_to_coordinates\u001b[39m(answer_coordinates_question):\n\u001b[1;32m-> 1734\u001b[0m     \u001b[39mreturn\u001b[39;00m [(coords[\u001b[39m1\u001b[39m], coords[\u001b[39m0\u001b[39m]) \u001b[39mfor\u001b[39;00m coords \u001b[39min\u001b[39;00m answer_coordinates_question]\n",
            "File \u001b[1;32mc:\\Users\\michi\\Anaconda3\\envs\\enterpriseai2\\lib\\site-packages\\transformers\\models\\tapas\\tokenization_tapas.py:1734\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1733\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_to_coordinates\u001b[39m(answer_coordinates_question):\n\u001b[1;32m-> 1734\u001b[0m     \u001b[39mreturn\u001b[39;00m [(coords[\u001b[39m1\u001b[39;49m], coords[\u001b[39m0\u001b[39m]) \u001b[39mfor\u001b[39;00m coords \u001b[39min\u001b[39;00m answer_coordinates_question]\n",
            "\u001b[1;31mTypeError\u001b[0m: 'int' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from transformers import TapasForQuestionAnswering, TapasTokenizer\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import torch\n",
        "\n",
        "# Load the pre-trained model and tokenizer\n",
        "model = TapasForQuestionAnswering.from_pretrained('google/tapas-base-finetuned-wtq')\n",
        "tokenizer = TapasTokenizer.from_pretrained('google/tapas-base-finetuned-wtq')\n",
        "\n",
        "# Prepare your own training data in the format expected by the model\n",
        "table = [\n",
        "    ['Name', 'Age', 'Gender'],\n",
        "    ['John', '25', 'Male'],\n",
        "    ['Jane', '30', 'Female']\n",
        "]\n",
        "\n",
        "\n",
        "df = pd.DataFrame(table[1:], columns=table[0])\n",
        "\n",
        "queries = [\"What's the Age of John?\"]\n",
        "answer_text = [\"25\"]\n",
        "answer_coordinates = [(1, 1, 1, 1)] \n",
        "\n",
        "\n",
        "# Tokenize the training data using the TapasTokenizer\n",
        "inputs = tokenizer(\n",
        "    table=df,\n",
        "    queries=queries,\n",
        "    answer_text=answer_text,\n",
        "    answer_coordinates=answer_coordinates,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          \n",
        "    num_train_epochs=3,           \n",
        "    per_device_train_batch_size=16, \n",
        "    per_device_eval_batch_size=64, \n",
        "    warmup_steps=500,                \n",
        "    weight_decay=0.01,            \n",
        "    logging_dir='./logs',           \n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "# Define the trainer\n",
        "trainer = Trainer(\n",
        "    model=model,                        \n",
        "    args=training_args,                 \n",
        "    train_dataset=inputs,               \n",
        "    data_collator=lambda data: {'input_ids': torch.stack([x['input_ids'] for x in data]),\n",
        "                                'attention_mask': torch.stack([x['attention_mask'] for x in data]),\n",
        "                                'token_type_ids': torch.stack([x['token_type_ids'] for x in data]),\n",
        "                                'labels': torch.stack([x['labels'] for x in data])},\n",
        "    tokenizer=tokenizer,                 # tokenizer to be used for tokenization\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "trainer.save_model('./fine-tuned-model')\n",
        "tokenizer.save_pretrained('./fine-tuned-tokenizer')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mlflow\n",
        "\n",
        "# Start an MLflow run\n",
        "mlflow.start_run()\n",
        "\n",
        "# Log your hyperparameters\n",
        "mlflow.log_params({\n",
        "    \"learning_rate\": 1e-5,\n",
        "    \"batch_size\": 32,\n",
        "    \"num_epochs\": 1\n",
        "})\n",
        "\n",
        "# Step 4: Model Training\n",
        "# ...\n",
        "\n",
        "# Set up your training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    # ...\n",
        "\n",
        "    for batch in DataLoader(dataset, batch_size=batch_size, shuffle=True):\n",
        "        # ...\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Log the loss metric\n",
        "        mlflow.log_metric(\"loss\", loss.item(), step=epoch)\n",
        "\n",
        "        # ...\n",
        "\n",
        "# Step 5: Fine-tuning Parameters\n",
        "# ...\n",
        "\n",
        "# Log your trained model as an artifact\n",
        "mlflow.pytorch.log_model(model, \"tapas_model\")\n",
        "\n",
        "# End the MLflow run\n",
        "mlflow.end_run()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Fine-tuning TapasForQuestionAnswering on SQA.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3bad5ab136084c2e92b55153828a403f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c95dd6249784fe6a2a466737e5f5866": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "45fdca8a04f94b43b021c590181e8a48": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "695d0c44ebbe4a55a17c735645c26c82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_be6159a5be0945629a517297df1170b8",
              "IPY_MODEL_b889ffa28a1949e9ac46e205ee582688"
            ],
            "layout": "IPY_MODEL_7e3cd6c49143436bad3d84be7ca2cf79"
          }
        },
        "768723f1af4c4bf497064a9796567382": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c6aadbda246d47fcad9660c2f57926a0",
              "IPY_MODEL_cd910003940b498e94db9746091e4c3c"
            ],
            "layout": "IPY_MODEL_82bfe5d88c5546358de33952e42221a3"
          }
        },
        "7e3cd6c49143436bad3d84be7ca2cf79": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82bfe5d88c5546358de33952e42221a3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a11bc2e0d34543d1bb0ab708e0cc1a67": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "b889ffa28a1949e9ac46e205ee582688": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3bad5ab136084c2e92b55153828a403f",
            "placeholder": "​",
            "style": "IPY_MODEL_45fdca8a04f94b43b021c590181e8a48",
            "value": " 443M/443M [00:06&lt;00:00, 66.0MB/s]"
          }
        },
        "be6159a5be0945629a517297df1170b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf9382ee988f4787a88340d3b2af95f3",
            "max": 442768791,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3c95dd6249784fe6a2a466737e5f5866",
            "value": 442768791
          }
        },
        "c6aadbda246d47fcad9660c2f57926a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d59737fa0a424a90ba02110e4cf1b639",
            "max": 1432,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a11bc2e0d34543d1bb0ab708e0cc1a67",
            "value": 1432
          }
        },
        "cd910003940b498e94db9746091e4c3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6fef10b29f74c458d05c2cbda46bf4a",
            "placeholder": "​",
            "style": "IPY_MODEL_d534c2ebdbb144efa59500fcadf6e118",
            "value": " 1.43k/1.43k [00:00&lt;00:00, 4.46kB/s]"
          }
        },
        "cf9382ee988f4787a88340d3b2af95f3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d534c2ebdbb144efa59500fcadf6e118": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d59737fa0a424a90ba02110e4cf1b639": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6fef10b29f74c458d05c2cbda46bf4a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
